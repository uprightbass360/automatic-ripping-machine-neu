#!/usr/bin/env python3
"""Collection of utility functions"""
import datetime
import json
import os
import logging
import subprocess
import shutil
import time
import random
import re
from logging import Logger
from pathlib import Path, PurePath
from math import ceil

import bcrypt


def extract_year(raw: str) -> str:
    """Extract a 4-digit year from strings like '2006-05-19', '2006–2008', '2006–'.

    Returns the first 4-digit sequence found, or the original string if none.
    """
    m = re.search(r"\d{4}", str(raw))
    return m.group(0) if m else raw
import httpx
import requests
import apprise
import psutil

from netifaces import interfaces, ifaddresses, AF_INET

import arm.config.config as cfg
from arm.ripper.ProcessHandler import arm_subprocess
from arm.database import db  # needs to be imported before models
from arm.models.job import Job, JobState
from arm.models.notifications import Notifications
from arm.models.track import Track
from arm.models.user import User
from arm.models.app_state import AppState
from arm.models.system_drives import SystemDrives
from arm.ripper import apprise_bulk

NOTIFY_TITLE = "ARM notification"


class RipperException(Exception):
    pass


def notify(job, title: str, body: str):
    """
    Send notifications with apprise\n
    :param job: Current Job
    :param title: title for notification
    :param body: body of the notification
    :return: None
    """

    # Prepend Site Name if configured
    if cfg.arm_config["ARM_NAME"] != "":
        title = f"[{cfg.arm_config['ARM_NAME']}] - {title}"

    # append Job ID if configured
    if cfg.arm_config["NOTIFY_JOBID"] and job is not None:
        title = f"{title} - {job.job_id}"

    # Send to local db
    logging.debug(f"apprise message, title: {title} body: {body}")
    notification = Notifications(title, body)
    database_adder(notification)

    bash_notify(cfg.arm_config, title, body, job)
    transcoder_notify(cfg.arm_config, title, body, job)

    # Sent to remote sites
    # Create an Apprise instance
    apobj = apprise.Apprise()
    if cfg.arm_config["PB_KEY"] != "":
        apobj.add('pbul://' + str(cfg.arm_config["PB_KEY"]))
    if cfg.arm_config["IFTTT_KEY"] != "":
        apobj.add('ifttt://' + str(cfg.arm_config["IFTTT_KEY"]) + "@" + str(cfg.arm_config["IFTTT_EVENT"]))
    if cfg.arm_config["PO_USER_KEY"] != "":
        apobj.add('pover://' + str(cfg.arm_config["PO_USER_KEY"]) + "@" + str(cfg.arm_config["PO_APP_KEY"]))
    if cfg.arm_config["JSON_URL"] != "":
        apobj.add(str(cfg.arm_config["JSON_URL"]).replace("http://", "json://").replace("https://", "jsons://"))
    if len(apobj) > 0:
        try:
            apobj.notify(body, title=title)
        except Exception as error:  # noqa: E722
            logging.error(f"Failed sending notifications. error:{error}. Continuing processing...")

    # Bulk send notifications, using the config set on the ripper config page
    if cfg.arm_config["APPRISE"] != "":
        try:
            apprise_bulk.apprise_notify(cfg.arm_config["APPRISE"], title, body)
            logging.debug(f"apprise-config: {cfg.arm_config['APPRISE']}")
        except Exception as error:  # noqa: E722
            logging.error(f"Failed sending apprise notifications. {error}")


def bash_notify(cfg, title, body, job=None):
    """Run BASH_SCRIPT with notification data.
    Positional args ($1=title, $2=body) preserved for backward compatibility.
    Job metadata passed via ARM_* environment variables."""
    if cfg['BASH_SCRIPT'] != "":
        try:
            env = os.environ.copy()
            if job is not None:
                env['ARM_JOB_ID'] = str(job.job_id or '')
                env['ARM_TITLE'] = str(job.title or '')
                env['ARM_TITLE_AUTO'] = str(job.title_auto or '')
                env['ARM_YEAR'] = str(job.year or '')
                env['ARM_VIDEO_TYPE'] = str(job.video_type or '')
                env['ARM_DISCTYPE'] = str(job.disctype or '')
                env['ARM_LABEL'] = str(job.label or '')
                env['ARM_STATUS'] = str(job.status or '')
                env['ARM_PATH'] = str(job.path or '')
                env['ARM_RAW_PATH'] = str(job.raw_path or '')
                env['ARM_TRANSCODE_PATH'] = str(job.transcode_path or '')
                if job.config:
                    env['ARM_RAW_PATH_BASE'] = str(getattr(job.config, 'RAW_PATH', '') or '')
                    env['ARM_COMPLETED_PATH_BASE'] = str(getattr(job.config, 'COMPLETED_PATH', '') or '')
            subprocess.run(["/usr/bin/env", "bash", cfg['BASH_SCRIPT'], title, body], env=env)
            logging.debug("Sent bash notification successful")
        except Exception as error:  # noqa: E722
            logging.error(f"Failed sending notification via bash. Continuing  processing...{error}")


def transcoder_notify(cfg, title, body, job=None):
    """Send a webhook notification to the arm-transcoder service.
    If LOCAL_RAW_PATH and SHARED_RAW_PATH are both set, moves the job's
    raw directory from local to shared storage before notifying."""
    transcoder_url = cfg.get('TRANSCODER_URL', '')
    if not transcoder_url:
        return

    # Move files from local scratch to shared storage if configured
    local_raw = cfg.get('LOCAL_RAW_PATH', '')
    shared_raw = cfg.get('SHARED_RAW_PATH', '')
    raw_basename = ''

    if job is not None and job.raw_path:
        raw_basename = os.path.basename(str(job.raw_path))

    if local_raw and shared_raw and raw_basename:
        src = os.path.join(local_raw, raw_basename)
        dst = os.path.join(shared_raw, raw_basename)
        if os.path.isdir(src):
            try:
                os.makedirs(shared_raw, exist_ok=True)
                shutil.move(src, dst)
                logging.info(f"Moved {src} -> {dst}")
            except OSError as e:
                logging.error(f"Failed to move {src} -> {dst}: {e}")

    # Build payload
    payload = {
        "title": title,
        "body": body,
        "type": "info",
    }
    if raw_basename:
        payload["path"] = raw_basename
    if job is not None:
        payload["job_id"] = str(job.job_id)
        payload["video_type"] = str(job.video_type or '')
        payload["year"] = str(job.year or '')
        payload["disctype"] = str(job.disctype or '')
        payload["status"] = str(job.status or '')
        payload["poster_url"] = str(job.poster_url or '')
        if job.transcode_overrides:
            try:
                payload["config_overrides"] = json.loads(job.transcode_overrides)
            except (json.JSONDecodeError, TypeError):
                pass

    # Send webhook
    headers = {"Content-Type": "application/json"}
    secret = cfg.get('TRANSCODER_WEBHOOK_SECRET', '')
    if secret:
        headers["X-Webhook-Secret"] = secret

    try:
        with httpx.Client(timeout=10) as client:
            resp = client.post(transcoder_url, json=payload, headers=headers)
        if resp.status_code in (401, 403):
            logging.error(
                f"Transcoder webhook auth failed (HTTP {resp.status_code}). "
                "Check TRANSCODER_WEBHOOK_SECRET matches WEBHOOK_SECRET on the transcoder."
            )
        else:
            logging.info(f"Transcoder webhook sent (HTTP {resp.status_code})")
    except Exception as e:
        logging.error(f"Failed sending transcoder webhook: {e}")


def notify_entry(job):
    """
    Notify On Entry\n
    :param job:
    :return: None
    """
    # TODO make this better or merge with notify/class
    notification = Notifications(f"New Job: {job.job_id} has started. Disctype: {job.disctype}",
                                 f"New job has started to rip - {job.label},"
                                 f"{job.disctype} at {datetime.datetime.now()}")
    database_adder(notification)
    if job.disctype in ["dvd", "bluray", "bluray4k"]:
        if cfg.arm_config["UI_BASE_URL"] == "":
            display_address = (f"http://{check_ip()}:{job.config.WEBSERVER_PORT}")
        else:
            display_address = str(cfg.arm_config["UI_BASE_URL"])
        # Send the notifications
        notify(job, NOTIFY_TITLE,
               f"Found disc: {job.title}. Disc type is {job.disctype}. Main Feature is {job.config.MAINFEATURE}."
               f"Edit entry here: {display_address}/jobdetail?job_id={job.job_id}")
    elif job.disctype == "music":
        notify(job, NOTIFY_TITLE, f"Found music CD: {job.label}. Ripping all tracks.")
    elif job.disctype == "data":
        notify(job, NOTIFY_TITLE, "Found data disc.  Copying data.")
    else:
        raise RipperException("Could not determine disc type")


def sleep_check_process(process_str, max_processes, sleep=(20, 120, 10)):
    """
    New function to check for max_transcode from job.config and force obey limits\n
    :param str process_str: The process string from arm.yaml
    :param int max_processes: The user defined limit for maximum transcodes
    :param (tuple, int) sleep: tuple: (min sleep time, max sleep time, step) or sleep time as int.
    :return bool: when we have space in the transcode queue
    """
    if max_processes <= 0:
        return False  # sleep limit disabled
    if isinstance(sleep, int):
        sleep = (sleep, sleep + 1, 1)
    if not isinstance(sleep, tuple):
        raise TypeError(sleep)
    loop_count = max_processes + 1
    logging.info(f"Starting sleep check of {process_str}")
    while loop_count >= max_processes:
        # The process might disappear during loops, so we need to query the
        # name upfront.
        loop_count = sum(
            1 for proc in psutil.process_iter(['name'])
            if proc.info.get('name') == process_str
        )
        if max_processes > loop_count:
            break
        # Try to make each check at different times
        random_time = random.randrange(*sleep)
        logging.debug(f"{loop_count} processes running. Sleeping for {random_time}s.")
        time.sleep(random_time)
    logging.info(f"Exiting sleep check of {process_str}")
    return True


def scan_emby():
    """Trigger a media scan on Emby"""

    if cfg.arm_config["EMBY_REFRESH"]:
        logging.info("Sending Emby library scan request")
        url = f"http://{cfg.arm_config['EMBY_SERVER']}:{cfg.arm_config['EMBY_PORT']}/Library/Refresh?api_key={cfg.arm_config['EMBY_API_KEY']}"  # noqa: E501
        try:
            req = requests.post(url)
            if req.status_code > 299:
                req.raise_for_status()
            logging.info("Emby Library Scan request successful")
        except requests.exceptions.HTTPError:
            logging.error(f"Emby Library Scan request failed with status code: {req.status_code}")
    else:
        logging.info("EMBY_REFRESH config parameter is false.  Skipping emby scan.")


def delete_raw_files(dir_list):
    """
    Delete the raw folders from arm after job has finished
    :param list dir_list: Python list containing strings of the folders to be deleted

    """
    if cfg.arm_config["DELRAWFILES"]:
        for raw_folder in dir_list:
            try:
                logging.info(f"Removing raw path - {raw_folder}")
                shutil.rmtree(raw_folder)
            except UnboundLocalError as error:
                logging.debug(f"No raw files found to delete in {raw_folder}- {error}")
            except OSError as error:
                logging.debug(f"No raw files found to delete in {raw_folder} - {error}")
            except TypeError as error:
                logging.debug(f"No raw files found to delete in {raw_folder} - {error}")


def make_dir(path: str, exist_ok: bool = True) -> bool:
    """
    Make a directory\n
    :param path: Path to directory
    :param exist_ok: If ``True``, simply returns ``False`` in case the
        directory exists. If ``False``, raises a ``RipperException`` in that case.
    :return: ``True`` if the directory was created, ``False`` if it already existed
    :raises: ``RipperException``
    """
    try:
        os.makedirs(path)
        logging.debug(f"Created directory: {path}")
        return True
    except FileExistsError as err:
        if exist_ok:
            return False
        else:
            raise RipperException(f"Folder exists: {path}") from err
    except OSError as err:
        raise RipperException(f"Could not create folder: {path}") from err


def find_file(filename, search_path):
    """
    Check to see if file exists by searching a directory recursively\n
    :param filename: filename to look for
    :param search_path: path to search recursively
    :return bool:
    """
    for dirpath, dirnames, filenames in os.walk(search_path):
        if filename in filenames:
            return True
    return False


def rip_music(job, logfile):
    """
    Rip music CD using abcde config\n
    :param job: job object
    :param logfile: location of logfile\n
    :return: Bool on success or fail
    """

    abcfile = cfg.arm_config["ABCDE_CONFIG_FILE"]
    if job.disctype == "music":
        logging.info("Disc identified as music")
        # If user has set a cfg.arm_config file with ARM use it
        if os.path.isfile(abcfile):
            cmd = f'abcde -d "{job.devpath}" -c {abcfile} >> "{os.path.join(job.config.LOGPATH, logfile)}" 2>&1'
        else:
            cmd = f'abcde -d "{job.devpath}" >> "{os.path.join(job.config.LOGPATH, logfile)}" 2>&1'

        logging.debug(f"Sending command: {cmd}")
        args = {"status": JobState.AUDIO_RIPPING.value}
        database_updater(args, job)

        try:
            subprocess.check_output(cmd, shell=True).decode("utf-8")
            # abcde exits 0 even on drive I/O errors — check the log for failures
            logpath = os.path.join(job.config.LOGPATH, logfile)
            try:
                with open(logpath, "r", errors="replace") as f:
                    log_content = f.read()
            except OSError as e:
                logging.warning(f"Could not read abcde log {logpath}: {e}")
                log_content = ""
            error_lines = re.findall(r"^\[ERROR\].*$", log_content, re.MULTILINE)
            if not error_lines and "CDROM drive unavailable" in log_content:
                error_lines = ["CDROM drive unavailable"]
            if error_lines:
                err = "; ".join(error_lines)
                logging.error(err)
                args = {"status": JobState.FAILURE.value, "errors": err}
                database_updater(args, job)
                return False
            logging.info("abcde call successful")
            args = {"status": JobState.IDLE.value}
            database_updater(args, job)
            return True
        except subprocess.CalledProcessError as ab_error:
            err = f"Call to abcde failed with code: {ab_error.returncode} ({ab_error.output})"
            args = {"status": JobState.FAILURE.value, "errors": err}
            database_updater(args, job)
            logging.error(err)
    return False


def rip_data(job):
    """
    Rip data disc using dd on the command line\n
    :param job: Current job
    :return: True/False for success/fail
    """
    success = False
    if job.label == "" or job.label is None:
        job.label = "data-disc"
    # get filesystem in order
    raw_path = os.path.join(job.config.RAW_PATH, str(job.label))
    type_sub_folder = job.type_subfolder if hasattr(job, 'type_subfolder') else "unidentified"
    final_path = os.path.join(job.config.COMPLETED_PATH, type_sub_folder)
    final_file_name = str(job.label)

    if (make_dir(raw_path)) is False:
        random_time = str(round(time.time() * 100))
        raw_path = os.path.join(job.config.RAW_PATH, str(job.label) + "_" + random_time)
        final_file_name = f"{job.label}_{random_time}"
        make_dir(raw_path, False)

    final_path = os.path.join(final_path, final_file_name)
    incomplete_filename = os.path.join(raw_path, final_file_name + ".part")
    make_dir(final_path)
    logging.info(f"Ripping data disc to: {incomplete_filename}")
    # Added from pull 366
    cmd = f'dd if="{job.devpath}" of="{incomplete_filename}" {cfg.arm_config["DATA_RIP_PARAMETERS"]} 2>> ' \
          f'{os.path.join(job.config.LOGPATH, job.logfile)}'
    logging.debug(f"Sending command: {cmd}")
    try:
        subprocess.check_output(cmd, shell=True).decode("utf-8")
        full_final_file = os.path.join(final_path, f"{final_file_name}.iso")
        logging.info(f"Moving data-disc from '{incomplete_filename}' to '{full_final_file}'")
        if not os.path.isfile(full_final_file):
            try:
                shutil.move(incomplete_filename, full_final_file)
            except Exception as error:
                logging.error(f"Unable to move '{incomplete_filename}' to '{final_path}' - Error: {error}")
        else:
            logging.info(f"File: {full_final_file} already exists.  Not moving.")
        logging.info("Data rip call successful")
        database_updater({'path': full_final_file}, job)
        success = True
    except subprocess.CalledProcessError as dd_error:
        err = f"Data rip failed with code: {dd_error.returncode}({dd_error.output})"
        logging.error(err)
        os.unlink(incomplete_filename)
        args = {"status": JobState.FAILURE.value, "errors": err}
        database_updater(args, job)
    try:
        logging.info(f"Trying to remove raw_path: '{raw_path}'")
        shutil.rmtree(raw_path)
    except OSError as error:
        logging.error(f"Error: {error.filename} - {error.strerror}.")
    return success


def try_add_default_user():
    """
    Added to fix missmatch from the armui and armripper\n
    This will try to add a default user for the armui
    with the details\n
    Username: admin\n
    Password: password\n
    :return: None
    """
    try:
        username = "admin"
        pass1 = "password".encode('utf-8')
        hashed = bcrypt.gensalt(12)
        database_adder(User(email=username, password=bcrypt.hashpw(pass1, hashed), hashed=hashed))
        perm_file = Path(PurePath(cfg.arm_config['INSTALLPATH'], "installed"))
        write_permission_file = open(perm_file, "w")
        write_permission_file.write("boop!")
        write_permission_file.close()
    except Exception as error:
        #  notify("", str(error), str(error))
        logging.error(error)


def put_track(job, t_no, seconds, aspect, fps, mainfeature, source, filename=""):
    """
    Put data into a track instance.\n
    Having this here saves importing the models file everywhere\n

    :param job: instance of job class
    :param str t_no: track number
    :param int seconds: length of track in seconds
    :param str aspect: aspect ratio (ie '16:9')
    :param str fps: frames per second:str (-not a float-)
    :param bool mainfeature: If the file is identified as the mainfeature
    :param str source: Source of information (HandBrake, MakeMKV, abcde)
    :param str filename: filename of track
    """

    logging.debug(
        f"Track #{int(t_no):02} Length: {seconds: >4} fps: {float(fps):2.3f} "
        f"aspect: {aspect: >4} Mainfeature: {mainfeature} Source: {source}")

    job_track = Track(
        job_id=job.job_id,
        track_number=t_no,
        length=seconds,
        aspect_ratio=aspect,
        fps=fps,
        main_feature=mainfeature,
        source=source,
        basename=job.title,
        filename=filename
    )
    job_track.ripped = (seconds > int(job.config.MINLENGTH))
    database_adder(job_track)


def arm_setup(arm_log: Logger) -> None:
    """
    Setup arm - Create all the directories we need for arm to run
    check that folders are writeable, and the db file is writeable
    """
    arm_directories = (
        cfg.arm_config['RAW_PATH'],
        cfg.arm_config['COMPLETED_PATH'],
        cfg.arm_config['LOGPATH'],
        os.path.join(cfg.arm_config['LOGPATH'], "progress"),
    )
    # Check if DB file is writeable
    if not os.access(cfg.arm_config['DBFILE'], os.W_OK):
        arm_log.critical(f"Can't write to database file: {cfg.arm_config['DBFILE']}")
    # Check directories for read/write permission -> create if they don't exist
    for folder in arm_directories:
        os.makedirs(folder, exist_ok=True)
        if not os.access(folder, os.R_OK):
            arm_log.error(f"Can't read from folder: {folder}")
        if not os.access(folder, os.W_OK):
            arm_log.critical(f"Can't write to folder: {folder}")


def database_updater(args, job, wait_time=90):
    """
    Try to update our db for x seconds and handle it nicely if we can't
    If args isn't a dict assume we are wanting a rollback\n

    :param args: This needs to be a Dict with the key being the job.method
    you want to change and the value being
    the new value.
    :param job: This is the job object
    :param int wait_time: Number of times to try(1 sec sleep between try)
    :return: Success
    """
    if not isinstance(args, dict):
        db.session.rollback()
        return False
    # Loop through our args and try to set any of our job variables
    _sensitive_keys = {'api_key', 'arm_api_key', 'omdb_api_key', 'tmdb_api_key',
                       'pb_key', 'ifttt_key', 'po_user_key', 'po_app_key', 'apprise'}
    for (key, value) in args.items():
        setattr(job, key, value)
        safe_key = str(key)
        if safe_key.lower() in _sensitive_keys:
            logging.debug("ID:%s %s=<redacted>", int(job.job_id), safe_key)
        else:
            logging.debug("ID:%s %s=%s:%s", int(job.job_id), safe_key, str(value), type(value).__name__)

    for i in range(wait_time):  # give up after the users wait period in seconds
        try:
            db.session.commit()
            break
        except Exception as error:
            if "locked" in str(error):
                time.sleep(1)
                logging.debug(f"database is locked - try {i}/{wait_time}")
            else:
                logging.debug(f"Error: {error}")
                raise RuntimeError(str(error)) from error
    logging.debug("successfully written to the database")
    return True


def database_adder(obj_class):
    """
    Adds model item to db\n
    Used to stop database locked error\n
    :param obj_class: Job/Config/Track/ etc
    :return: True if success
    """
    for i in range(90):  # give up after the users wait period in seconds
        try:
            logging.debug(f"Trying to add {type(obj_class).__name__}")
            db.session.add(obj_class)
            db.session.commit()
            break
        except Exception as error:
            if "locked" in str(error):
                time.sleep(1)
                logging.debug(f"database is locked - try {i}/90")
            else:
                logging.error(f"Error: {error}")
                raise RuntimeError(str(error)) from error
    logging.debug(f"successfully written {type(obj_class).__name__} to the database")
    return True


def clean_old_jobs():
    """
    Check for running jobs - Update failed jobs that are no longer running\n
    :return: None
    """
    active_jobs = db.session.query(Job).filter(Job.status.notin_(['fail', 'success'])).all()
    # Clean up abandoned jobs
    for job in active_jobs:
        if psutil.pid_exists(job.pid):
            job_process = psutil.Process(job.pid)
            if job.pid_hash == hash(job_process):
                logging.info(f"Job #{job.job_id} with PID {job.pid} is currently running.")
        else:
            logging.info(f"Job #{job.job_id} with PID {job.pid} has been abandoned."
                         f"Updating job status to fail.")
            job.status = JobState.FAILURE.value
            db.session.commit()
            database_updater({'status': JobState.FAILURE.value}, job)


def check_ip():
    """
        Check if user has set an ip in the config file
        if not gets the most likely ip
        arguments:
        none
        return: the ip of the host or 127.0.0.1
    """
    if cfg.arm_config['WEBSERVER_IP'] != 'x.x.x.x':
        return cfg.arm_config['WEBSERVER_IP']
    # autodetect host IP address
    ip_list = []
    for interface in interfaces():
        inet_links = ifaddresses(interface).get(AF_INET, [])
        for link in inet_links:
            ip_address = link['addr']
            if ip_address != '127.0.0.1' and not ip_address.startswith('172'):
                ip_list.append(ip_address)
    if len(ip_list) > 0:
        return ip_list[0]
    return '127.0.0.1'


def clean_for_filename(string):
    """ Cleans up string for use in filename """
    string = re.sub('\\[(.*?)]', '', string)
    string = re.sub('\\s+', '-', string)
    string = string.replace(' : ', ' - ')
    string = string.replace(':', '-')
    string = string.replace('&', 'and')
    string = string.replace("\\", " - ")
    string = string.replace(" ", " - ")
    string = string.strip()
    return re.sub('[^\\w.() -]', '', string)


def duplicate_run_check(dev_path):
    """
    Kills this run if another run was triggered recently on the same device\n
    Some drives will trigger the udev twice causing 1 disc insert to add 2 jobs\n
    this stops that issue
    :return: None
    """
    # Log running jobs by job status
    running_jobs = (
        db.session.query(Job)
        .filter(
            ~Job.finished,
            Job.devpath == dev_path,
        )
        .all()
    )
    for job in running_jobs:
        logging.info(f"Device {dev_path}: Job ({job.job_id}) status '{job.status}'")
    # check for running jobs by associated drive.
    drive = SystemDrives.query.filter_by(mount=dev_path).first()
    if not drive.processing:
        return  # drive is not processing, so we are safe to start another run.
    job = drive.job_current
    logging.critical(f'Drive {dev_path} has an active Job ({job.job_id}): {job.status}.')
    # log time
    job_time = ceil(job.run_time // 60)
    logging.info(f"Job was started {job_time}min ago.")
    if (job_time) < 3:
        logging.info("Job was started less than 3min ago.")
    raise RipperException(f"Job already running on {dev_path}")


def save_disc_poster(final_directory, job):
    """
     Use FFMPeg to convert Large Poster if enabled in config
    :param final_directory: folder to put the poster in
    :param job: Current Job
    :return: None
    """
    if job.disctype == "dvd" and cfg.arm_config["RIP_POSTER"]:
        os.system(f"mount {job.devpath}")
        if os.path.isfile(job.mountpoint + "/JACKET_P/J00___5L.MP2"):
            logging.info("Converting NTSC Poster Image")
            os.system(f'ffmpeg -i "{job.mountpoint}/JACKET_P/J00___5L.MP2" "{final_directory}/poster.png"')
        elif os.path.isfile(job.mountpoint + "/JACKET_P/J00___6L.MP2"):
            logging.info("Converting PAL Poster Image")
            os.system(f'ffmpeg -i "{job.mountpoint}/JACKET_P/J00___6L.MP2" "{final_directory}/poster.png"')
        os.system(f"umount {job.devpath}")


def check_for_dupe_folder(have_dupes, hb_out_path, job):
    """
    Check if the folder already exists
     if it exists lets make a new one using random numbers
    :param have_dupes: is this title in the local arm database
    :param hb_out_path: path to HandBrake out
    :param job: Current job
    :return: Final media directory path
    """
    if (make_dir(hb_out_path)) is False:
        logging.info(f"Output directory \"{hb_out_path}\" already exists.")
        # Only begin ripping if we are allowed to make duplicates
        # Or the successful rip of the disc is not found in our database
        logging.debug(f"Value of ALLOW_DUPLICATES: {cfg.arm_config['ALLOW_DUPLICATES']}")
        logging.debug(f"Value of have_dupes: {have_dupes}")
        if cfg.arm_config["ALLOW_DUPLICATES"] or not have_dupes:
            hb_out_path = hb_out_path + "_" + job.stage
            make_dir(hb_out_path, False)
        else:
            # We aren't allowed to rip dupes, notify and exit
            logging.warning(
                f"Duplicate disc detected: '{job.title}' (label={job.label}, "
                f"crc={job.crc_id}). Skipping — ALLOW_DUPLICATES is false."
            )
            notify(job, NOTIFY_TITLE, f"ARM Detected a duplicate disc. For {job.title}. "
                                      f"Duplicate rips are disabled. "
                                      f"You can re-enable them from your config file. ")
            raise RipperException("Duplicate rips are disabled")
    logging.info(f"Final Output directory \"{hb_out_path}\"")
    return hb_out_path


def job_dupe_check(job):
    """
    function for checking the database to look for jobs that have completed
    successfully with the same label
    :param job: The job obj, so we can use the crc/title etc.
    :return: True/False, dict/None
    """
    logging.debug(f"Trying to find jobs with matching Label={job.label}")
    if job.label is None:
        logging.info("Disc title 'None' not searched in database")
        return False
    else:
        previous_rips = Job.query.filter_by(label=job.label, status=JobState.SUCCESS.value)
        results = {}
        i = 0
        for j in previous_rips:
            # logging.debug(f"job obj= {j.get_d()}")
            job_dict = j.get_d().items()
            results[i] = {}
            for key, value in iter(job_dict):
                results[i][str(key)] = str(value)
            i += 1

    # logging.debug(f"previous rips = {results}")
    if results:
        logging.debug(f"we have {len(results)} jobs")
        # Check if results too large (over 1), skip if too many
        if len(results) == 1:
            # This might need some tweaks to because of title/year manual
            title = results[0]['title'] if results[0]['title'] else job.label
            year = extract_year(results[0]['year']) if results[0]['year'] != "" else ""
            poster_url = results[0]['poster_url'] if results[0]['poster_url'] != "" else None
            hasnicetitle = (str(results[0]['hasnicetitle']).lower() == 'true')
            video_type = results[0]['video_type'] if results[0]['hasnicetitle'] != "" else "unknown"
            active_rip = {
                "title": title, "year": year, "poster_url": poster_url, "hasnicetitle": hasnicetitle,
                "video_type": video_type}
            database_updater(active_rip, job)
            return True
        else:
            logging.debug(f"Skipping - There are too many results [{len(results)}]")
            return False
    else:
        logging.info("We have no previous rips/jobs matching this label")
        return False


def is_ripping_paused():
    """Check whether global ripping is paused via AppState.

    Uses a direct engine connection to bypass the session's open
    transaction, which may hold a stale SQLite read snapshot.
    """
    try:
        with db.engine.connect() as conn:
            row = conn.execute(
                db.text("SELECT ripping_paused FROM app_state WHERE id = 1")
            ).first()
            return bool(row[0]) if row else False
    except Exception:
        return False


def check_for_wait(job):
    """
    Wait if we have waiting for user input updates\n\n
    :param job: Current Job
    :return: None
    """
    globally_paused = is_ripping_paused()
    should_wait = job.config.MANUAL_WAIT or globally_paused

    if should_wait:
        if globally_paused:
            logging.info("Global ripping paused. Waiting for manual start.")
        else:
            logging.info(f"Waiting {job.config.MANUAL_WAIT_TIME} seconds for manual override.")
        database_updater({"status": JobState.MANUAL_WAIT_STARTED.value}, job)
        sleep_time = 0
        while True:
            time.sleep(5)
            sleep_time += 5
            db.session.refresh(job)

            # Check if job was cancelled externally
            if job.status != JobState.MANUAL_WAIT_STARTED.value:
                logging.info("Job status changed externally (cancelled). Aborting wait.")
                raise RipperException("Job was cancelled during manual wait.")

            # Re-check global pause and per-job pause each iteration
            paused_now = is_ripping_paused()
            job_paused = getattr(job, 'manual_pause', False) or False

            # User clicked "Start Ripping" — always honour, even when paused
            if job.manual_start:
                logging.info("Manual start triggered by user.")
                break

            # Per-job pause — hold this job indefinitely
            if job_paused:
                continue

            # User changed title — only proceed if not globally paused
            if job.title_manual and not paused_now:
                logging.info("Manual override found.  Overriding auto identification values.")
                job.updated = True
                job.hasnicetitle = True
                database_updater({"hasnicetitle": True, "updated": True}, job)
                break

            # Timeout only applies when not globally paused
            if not paused_now and sleep_time >= job.config.MANUAL_WAIT_TIME:
                logging.info("Manual wait time expired. Proceeding.")
                break

        database_updater({"status": JobState.IDLE.value}, job)


def get_drive_mode(devpath: str) -> str:
    """
    Retrieve the drive mode for a specified device path.

    This function queries the database for a drive associated with the provided
    device path (`devpath`). If a drive is found, it returns the drive's mode;
    otherwise, it defaults to 'auto'.

    Parameters:
        devpath (str): The device path used to identify the drive in the database.

    Returns:
        str: The drive mode associated with the specified device path if found;
             otherwise, returns 'auto'.
    """
    drive = SystemDrives.query.filter_by(mount=devpath).first()
    if drive:
        mode = drive.drive_mode
    else:
        mode = 'auto'
    return mode
